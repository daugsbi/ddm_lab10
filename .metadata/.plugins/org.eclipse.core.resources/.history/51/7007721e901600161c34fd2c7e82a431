package ch.fhnw.ddm.HadoopExercise.SaleAnalysis;

import java.io.DataOutputStream;
import java.io.File;
import java.io.IOException;
import java.nio.file.Files;
import java.text.DecimalFormat;

import org.apache.commons.io.FileUtils;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public static class SaleAnalysisFileOutputFormat extends FileOutputFormat<Text, DoubleWritable> { 
    @Override 
	public RecordWriter<Text, DoubleWritable> getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException { 
    	Path path = FileOutputFormat.getOutputPath(context);
	    Path fullPath = new Path(path, "result.txt"); 
	    
	    FileSystem fs = path.getFileSystem(context.getConfiguration()); 
	    FSDataOutputStream fileOut = fs.create(fullPath, context); 
	 
	    return new SaleAnalysisRecordWriter(fileOut); 
    } 
} 